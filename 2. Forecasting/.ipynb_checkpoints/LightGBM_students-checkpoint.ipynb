{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nf6nNvjl-Q4"
   },
   "source": [
    "https://github.com/minaxixi/Kaggle-M5-Forecasting-Accuracy/blob/master/model_recursive.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ha5kILeimTIL"
   },
   "source": [
    "This code is inspired by minaxixi code for Kaggle M5 competition, it uses LighGBM, gradient boosting regressor and recursive multistep forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "toAPM15KvIZo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGjjRr_FrRCs"
   },
   "source": [
    "Function to reduce memory usage of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jtnKRI5TwEHF"
   },
   "outputs": [],
   "source": [
    "\n",
    "def reduce_mem_usage(df, verbose=False):\n",
    "    '''\n",
    "    reduce memory usage by downcasting data types\n",
    "    from https://www.kaggle.com/harupy/m5-baseline\n",
    "    '''\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    int_columns = df.select_dtypes(include=[\"int\"]).columns\n",
    "    float_columns = df.select_dtypes(include=[\"float\"]).columns\n",
    "\n",
    "    for col in int_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "\n",
    "    for col in float_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZMuz_IurWpe"
   },
   "source": [
    "**Putting dataset of 5min for 1 year, which is imported from github depository**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2WIClnMwPG-",
    "outputId": "ffbf3ebc-e8c3-45a2-cdbb-cea8d7d78e84"
   },
   "outputs": [
    {
     "ename": "XLRDError",
     "evalue": "Excel xlsx file; not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXLRDError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-400e62e8e62c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m ).pipe(reduce_mem_usage, verbose=True)\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m                 )\n\u001b[0;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine)\u001b[0m\n\u001b[0;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\excel\\_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"xlrd\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\excel\\_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\xlrd\\__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[1;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows, ignore_workbook_corruption)\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;31m# files that xlrd can parse don't start with the expected signature.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfile_format\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfile_format\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'xls'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mXLRDError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFILE_FORMAT_DESCRIPTIONS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile_format\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'; not supported'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     bk = open_workbook_xls(\n",
      "\u001b[1;31mXLRDError\u001b[0m: Excel xlsx file; not supported"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('data.xlsx',\n",
    "sheet_name=0,\n",
    "header=0,\n",
    "index_col=[0],\n",
    "keep_default_na=True\n",
    ").pipe(reduce_mem_usage, verbose=True)\n",
    "\n",
    "#data = data[data.index.dayofweek < 5]\n",
    "#data = data.between_time('06:00','23:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "NvJ2aNG8HoL2",
    "outputId": "53b8d529-f7ef-4273-95eb-973dad5e63ac"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "_45f5oXMLtsQ",
    "outputId": "51ec7720-6976-4f5d-ae01-0cb8f7bb3c55"
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "He3e71PLHBeZ",
    "outputId": "430aeaa8-52a3-4820-9776-a8486a62dfb3"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kv0iD78trdpf"
   },
   "source": [
    "**Importing the calendar and creating a calendar dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4yrbyr9rSsaT",
    "outputId": "1a3da80e-2cf4-4803-942c-ed6a1af5645a"
   },
   "outputs": [],
   "source": [
    "!pip install icalendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42i_fjXVSp-W"
   },
   "outputs": [],
   "source": [
    "from icalendar import Calendar, Event\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtCsY9CDms36"
   },
   "source": [
    "In order to use the calendar you need to import it and place it in content folder, or just change the file location as you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sc9Na4pJm5vW"
   },
   "source": [
    "The main idea behind the calendar is to read it using the Icalendar library, and to extract all the information regarding the name of the event, and the starting and ending date. For some dates and one hour eventthere are multiple events, which I simply decided to keep the first appearing event (improvements could have been made by implementing a second column called \"parallel event\".\n",
    "Then the calendar dataframe must be suited to the dataframe. Since most events appear at sharp hours, and that the dataset is ssampled each 5min all the events are reported in the dataset event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RqiU0z9PS245",
    "outputId": "4b671ae4-1518-4b57-e711-805ac127dd24"
   },
   "outputs": [],
   "source": [
    "evenement = []\n",
    "debut = []\n",
    "fin =[]\n",
    "\n",
    "\n",
    "g = open('/content/stephane_stephane.ploix@gmail.com.ics','rb')\n",
    "gcal = Calendar.from_ical(g.read().decode())\n",
    "for component in gcal.walk():\n",
    "    if component.name == \"VEVENT\":\n",
    "\n",
    "        evenement.append(str((component.get('summary'))))\n",
    "        if len(str(component.get('dtstart').dt)) >12:\n",
    "            debut.append(datetime.strptime(str(component.get('dtstart').dt)[:-6],'%Y-%m-%d %H:%M:%S'))\n",
    "        else:\n",
    "            debut.append(datetime.strptime(str(component.get('dtstart').dt), '%Y-%m-%d'))\n",
    "        if component.get('dtend') is not None:\n",
    "            fin.append(component.get('dtend').dt)\n",
    "        else:\n",
    "            fin.append(\"Nan\")\n",
    "\n",
    "g.close()\n",
    "\n",
    "calendrier = pd.DataFrame({'evenement': evenement,'debut':debut,'fin':fin})\n",
    "calendrier['debut'] =pd.to_datetime(calendrier.debut)\n",
    "calendrier.sort_values(['debut'], inplace=True)\n",
    "calendrier = calendrier.set_index(calendrier['debut'])\n",
    "calendrier = calendrier['2015-01-04':'2015-12-31']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,9))\n",
    "calendrier.evenement.value_counts()[0:100].plot.bar()\n",
    "plt.show()\n",
    "\n",
    "label = []\n",
    "for k in calendrier.index:\n",
    "    if \"point\" in calendrier['evenement'].loc[str(k)]:\n",
    "        label.append(2)\n",
    "    else:\n",
    "        label.append(1)\n",
    "\n",
    "calendrier['label']=label\n",
    "print(calendrier.head())\n",
    "\n",
    "\n",
    "\n",
    "cal = []\n",
    "nom = []\n",
    "for k in data['label']:\n",
    "    cal.append(-1)\n",
    "    nom.append(\"None\")\n",
    "\n",
    "data['calendrier'] = cal\n",
    "data['nom']=nom\n",
    "\n",
    "calendrier.drop_duplicates(subset =\"debut\",\n",
    "                     keep = False, inplace = True)\n",
    "print(calendrier[calendrier.index.duplicated()])\n",
    "print(\"fin test\")\n",
    "\n",
    "for k in calendrier.index:\n",
    "    if k in data.index:\n",
    "        data['calendrier'].loc[str(k)] = calendrier['label'].loc[str(k)]\n",
    "        data['nom'].loc[str(k)] = calendrier['evenement'].loc[str(k)]\n",
    "\n",
    "plt.figure(figsize=(20,9))\n",
    "data['label'].plot()\n",
    "data['calendrier'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIC1DX4Gol3z"
   },
   "source": [
    "As we can see most of the events coincide with occupancy in the office, and a strong occupancy often matches with he \"point\" event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "id": "3RIABHF6c2An",
    "outputId": "801650db-3697-48a1-cf94-4ae3c0fa064a"
   },
   "outputs": [],
   "source": [
    "data[500:800]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "HhsBo0zWeUF4",
    "outputId": "d2d6ef0f-a566-43f4-9569-623e3ade0b1c"
   },
   "outputs": [],
   "source": [
    "calendrier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-4FdOfUowb8"
   },
   "source": [
    "**Data analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4Jii-x6o0Xl"
   },
   "source": [
    "Let's explore the data and check its time series properties. Since the pattern is mostly repeating each week we can plot the rolling mean over a week, a day and 10 days to have a general impression on trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "lrCHX8xHGk8q",
    "outputId": "cf440803-689e-4996-ce47-607f1e67a3f2"
   },
   "outputs": [],
   "source": [
    "plt.plot(data.index, data['label'])\n",
    "plt.plot(data.index, data['label'].rolling(24*12).mean(), label='1-day rolling mean')\n",
    "plt.plot(data.index, data['label'].rolling(7*24*12).mean(), label='1-day rolling mean')\n",
    "plt.plot(data.index, data['label'].rolling(10*24*12).mean(), label='1-day rolling mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAd9uS0DpIKi"
   },
   "source": [
    "We can observe that there is no particular trend but that the rollin mean over a week is almost linear on some timedate segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Mg3--V8pXTV"
   },
   "source": [
    "Now we decompose the data into trend, seasonal and residual. There is an important week seasonality and still a lot of noise which will make it difficult to have accurate forecasting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "R3hsbuA6HyH2",
    "outputId": "c2af463f-deb7-4728-85aa-2c59a70a270a"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "days_per_week = 7*24*12\n",
    "\n",
    "time_series = data[\"label\"]\n",
    "sj_sc = seasonal_decompose(time_series,freq = days_per_week)\n",
    "sj_sc.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEoH7to7pnRR"
   },
   "source": [
    "An important test regarding timeseries is the stationarity or not of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9D3X-BWIpcT"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "def test_stationarity(timeseries, window =24*12*7 ,cutoff = 0.01):\n",
    "\n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(window).mean()\n",
    "    rolstd = timeseries.rolling(window).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='{} Day Rolling Mean'.format(window))\n",
    "    std = plt.plot(rolstd, color='black', label = '{} Day Rolling Std'.format(window))\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show()\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC', maxlag = 20)\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    pvalue = dftest[1]\n",
    "    if pvalue < cutoff:\n",
    "        print('p-value = %.4f. The series is likely stationary.' % pvalue)\n",
    "    else:\n",
    "        print('p-value = %.4f. The series is likely non-stationary.' % pvalue)\n",
    "    \n",
    "    print(dfoutput)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "id": "K5Yugx4xIxNo",
    "outputId": "634c3366-feed-4b7c-b7a0-ee5573fc7f87"
   },
   "outputs": [],
   "source": [
    "test_stationarity(time_series, 7*24*12, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilgLJnAi0j92"
   },
   "source": [
    "The p-value is really small confirming that we have a stationary time series. This confirm our intent to test also the ARIMA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riF6ff-Q0uPH"
   },
   "source": [
    "In order to improve the accuracy of the model we give as an input feature the data characteristics since the model doesn't explicitly use the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WCvBzcsKMzI"
   },
   "outputs": [],
   "source": [
    "data.insert(0,'hour',data.index.hour)\n",
    "data.insert(0,'dayofweek',data.index.dayofweek)\n",
    "data.insert(0,'dayofmonth',data.index.day)\n",
    "data.insert(0,'year',data.index.year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "lvRKx-7CLwo2",
    "outputId": "90342ef6-926d-4f59-9010-680d36ae7930"
   },
   "outputs": [],
   "source": [
    "plt.plot(data.index, (data['label'].rolling(7*24*12).mean() / data['label'].rolling(5*17*12).mean().mean() - 1)*100)\n",
    "plt.legend('% change to the mean')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-O7jaczlnIs"
   },
   "source": [
    "**Feature engineering to create inputs based on statistics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LJsYl0s1Ada"
   },
   "source": [
    "In this part we are going to implement more sophisticated feature engineering. In stead of directly giving a sequence of the past data as it was done for the different neural networks we going to give statistical features that we observed above such as the rolling mean, the standard deviation or the maximum over a certain amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf16X7zw1e86"
   },
   "source": [
    "We try to give information about the last day, the last week, the last month and the last 2 months . A lot if different combinations were tried since we are doing a feature importance study later. We saw for NN models the calendar feature was not giving any improvement in the model, here we tried to also give statistical characteristics as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9waT3X-aN5gK"
   },
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    '''\n",
    "    create features using rolling and lag\n",
    "    '''\n",
    "    \n",
    "    # these are hard-coded for final submission\n",
    "    # groupby_cols, target_cols, agg_functions, rolling_windows, lags\n",
    "    agg_list = [\n",
    "        [['time'], 'label', 'mean', [12*24,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,50,100,150], [1,2,3,4,5,10,15,20,25,7*12*17,14*12*17]],\n",
    "        [['time'], 'label', 'mean', [7*12*24], [7*12*24, 365*12*24]],\n",
    "        [['time'], 'label', 'mean', [7*12*24, 14*12*24, 30*12*24, 60*12*24, 180], [28]],\n",
    "        [['time'], 'label', 'std',  [7*12*24, 14*12*24, 30*12*24, 60*12*24], [28*12*17]],\n",
    "        [['time'], 'label', 'max', [365*12*24], [1*12*24]],\n",
    "        [['time'], 'label', 'mean', [7*12*24], [1*12*24]],\n",
    "        [['time'], 'calendrier', 'mean', [12*24,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,50], [1,2,3,4,5,10,15,20,25,7*12*17,14*12*17]],\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    for i, item in enumerate(agg_list):\n",
    "        print(i)\n",
    "        \n",
    "        # unpack the parameters\n",
    "        groupby_cols, target_col, agg_function, rolling_windows, lags = item\n",
    "        groupby_col_str = \"_\".join(groupby_cols)\n",
    "        \n",
    "        for lag in lags:\n",
    "            for rolling_window in rolling_windows:\n",
    "                col_name = agg_function+'_'+target_col+'_per_'+groupby_col_str+'_r'+str(rolling_window)+'_lag_'+str(lag)\n",
    "                #df[col_name] = df[target_col].apply(lambda x: x.rolling(window=rolling_window).agg(agg_function).shift(lag))\n",
    "                df[col_name] = df[target_col].rolling(rolling_window).agg(agg_function).shift(lag)\n",
    "    ## price related features\n",
    "    #df['sell_price_norm'] = df['sell_price'] / df['max_sell_price_per_store_id_item_id_r365_lag_1']\n",
    "    #df['sell_price_momentum'] = df['sell_price'] / df['mean_sell_price_per_store_id_item_id_r7_lag_1']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3kJz10dlubB"
   },
   "source": [
    "**Function to create the future data input**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ8FsOL02NRm"
   },
   "source": [
    "Here we are creating a sequence of the calendar for the timesteps we are trying to forecast. Since the model took some time we did not perform a sensitivity analysis, probably we could have got better results if using 1 day instead of using 2 days as seen in the other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-a0K0LkW7vnF"
   },
   "outputs": [],
   "source": [
    "def calendar_feature(df):\n",
    "  k = range(1,12*24*2)\n",
    "  agg_list = [\n",
    "        [['time'], 'calendrier',k,\n",
    "\n",
    "    ]]\n",
    "    \n",
    "  for i, item in enumerate(agg_list):\n",
    "        print(i)\n",
    "        \n",
    "        # unpack the parameters\n",
    "        groupby_cols, target_col, lags = item\n",
    "        groupby_col_str = \"_\".join(groupby_cols)\n",
    "        \n",
    "        for lag in lags:\n",
    "                col_name = '_'+target_col+'_per_'+groupby_col_str+'_r''-_lag_'+str(lag)\n",
    "                #df[col_name] = df[target_col].apply(lambda x: x.rolling(window=rolling_window).agg(agg_function).shift(lag))\n",
    "                df[col_name] = df[target_col].shift(-lag)\n",
    "    ## price related features\n",
    "    #df['sell_price_norm'] = df['sell_price'] / df['max_sell_price_per_store_id_item_id_r365_lag_1']\n",
    "    #df['sell_price_momentum'] = df['sell_price'] / df['mean_sell_price_per_store_id_item_id_r7_lag_1']\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2yOj8J73JbX"
   },
   "source": [
    "Since this input did not improved the accuracy we didn't use it in the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5OgCmf72k5I"
   },
   "source": [
    "Here we create the dataset with past data input features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtW9LjBtPebt",
    "outputId": "83b84a89-68ab-4c9b-c4c1-53bdd5f8b739"
   },
   "outputs": [],
   "source": [
    "data2 = create_features(data)\n",
    "#data2 = calendar_feature(data2)\n",
    "#un hashtag if you want to test the calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "qaTn1QDuaXuh",
    "outputId": "06a4418d-2416-49c1-d062-88058ec0121a"
   },
   "outputs": [],
   "source": [
    "data2[1000:5000].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HfoWrNA2x_A"
   },
   "source": [
    "It results into a lot of columns in the dataframe, to improve the speed of our next algorithm lines we are reducing the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eFd4_4SUoU--",
    "outputId": "0ca92bf6-5d70-4263-f0da-38f4e87c10fb"
   },
   "outputs": [],
   "source": [
    "data3 = reduce_mem_usage(data2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWmkaRce28u6"
   },
   "source": [
    "We are reducing the time to access the dataset by saving it as pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KaJottT8oZPG"
   },
   "outputs": [],
   "source": [
    "data3.to_pickle(\"/content.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbiHZEQhpVE_"
   },
   "outputs": [],
   "source": [
    "sales_by_date = pd.read_pickle(\"/content.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3kZRt_0HBO5"
   },
   "outputs": [],
   "source": [
    "sales_by_date['label']=sales_by_date['label'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "id": "H8MqQMrnpdDu",
    "outputId": "e3a81a56-c880-42c8-ef9a-66b565441161"
   },
   "outputs": [],
   "source": [
    "sales_by_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjnctQyR3a7H"
   },
   "source": [
    "Let's chose the columns we want to predict and the ones we don't wish to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOpP5hsDptmt"
   },
   "outputs": [],
   "source": [
    "\n",
    "unused_cols = ['occupancy','Toffice_reference','humidity','detected_motions','office_CO2_concentratio','door','label','nom']\n",
    "\n",
    "feature_cols = list(set(sales_by_date.columns) - set(unused_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "visss6FTqSxt",
    "outputId": "e350c503-54a6-4ddd-8a1d-c9f3b517ebb7"
   },
   "outputs": [],
   "source": [
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuOE1ebnqcMR"
   },
   "outputs": [],
   "source": [
    "label_col = ['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dL88_drOqpXx"
   },
   "source": [
    "**Hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gD6EUpBh3in8"
   },
   "source": [
    "In order to train and test our model we are using a bit more than 75% for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dfg8zEYdqrb6"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = sales_by_date[feature_cols][0 : 80000]\n",
    "y_train = sales_by_date[label_col][0:80000]\n",
    "\n",
    "X_test = sales_by_date[feature_cols][80000:]\n",
    "y_test = sales_by_date[label_col][80000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "id": "GFOTCdpdrNfS",
    "outputId": "3681164f-c4b7-4493-bed6-0038a0c522f5"
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WaGzPGIErEiy",
    "outputId": "ea4a07f4-29bf-4228-a927-81d36d3abdeb"
   },
   "outputs": [],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nk3aFtayrH2f",
    "outputId": "8c086a6c-ec49-435c-bd2b-a5786ec92103"
   },
   "outputs": [],
   "source": [
    "print(\"train\", X_train.shape, y_train.shape)\n",
    "print(\"test\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-B6kTdBPsSeU",
    "outputId": "66d58907-a0d7-4cd8-cf80-6fa3cb6d631f"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFyJNBf53sz4"
   },
   "source": [
    "Now we are going to proceed to an hypertuning using 3 folds cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-T3M4a-i356e"
   },
   "source": [
    "It is important to note that the lightgbm can't make multioutput regression. That is why we are training the model to always forecast the one next step. In order to run correctly the prediction, we are using recursive prediction. That means that we are using a loop in which for the 24 hours we are going to update with the last predictions we made instead of giving the true value. This will be studier later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbcYVgPGGeyy"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAhdRWXDGgIC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "## define a 3-fold time-series split for cross validation\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTcGMh6FGksK"
   },
   "outputs": [],
   "source": [
    "## the parameter table for tuninig\n",
    "\n",
    "param_dist = {\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'objective': ['tweedie'],\n",
    "    'tweedie_variance_power': [1.1],\n",
    "    'n_estimators': [500],\n",
    "    'metric': ['rmse'],\n",
    "    'max_depth': [30, 50, 70],\n",
    "    'num_leaves': [250, 500, 1000],\n",
    "    'learning_rate': [0.03, 0.1],\n",
    "    'feature_fraction': [0.5, 0.7],\n",
    "    'bagging_fraction': [0.5, 0.7],\n",
    "}\n",
    "\n",
    "reg = lgb.LGBMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqdwN0EWGnfP"
   },
   "outputs": [],
   "source": [
    "## Set n_iter_search to be a higher value for actual hyperparameter tuning\n",
    "## This is just to show the workflow\n",
    "\n",
    "n_iter_search = 1\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    reg,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=n_iter_search,\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVL2tNJN4tBZ"
   },
   "source": [
    "Since we are studying a lot of features and making cross validation, this part takes about 20minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZW7HJ0YjGrUR",
    "outputId": "36274e8c-8058-472b-db5f-e8ed1b34c0cd"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Train on the training portion of the CV\n",
    "## Validated on the validation/test portion of the CV\n",
    "## Early stopping using the test portion of the dataset\n",
    "\n",
    "random_search.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_metric='rmse',\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydqdVxnLH9Ej"
   },
   "outputs": [],
   "source": [
    "## take the best model from our randomized search\n",
    "\n",
    "reg_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "liDg9WkmIAKV",
    "outputId": "ffec1a40-bacf-4daa-faf3-28b50fa19cf8"
   },
   "outputs": [],
   "source": [
    "## examine the feature importane\n",
    "\n",
    "\n",
    "lgb.plot_importance(reg_model, figsize=(10, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nl-e6s3zDpjg"
   },
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "id": "iNpoMHEyHavN",
    "outputId": "9b5d1f52-4d99-400e-acdb-f4e4f841ece1"
   },
   "outputs": [],
   "source": [
    "sales_by_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YH4fUmy05F5l"
   },
   "source": [
    "As we told before in order to make a multistep prediction we need a function that updates the features with the past single output prediction that was made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3rn1aMP8NT4"
   },
   "outputs": [],
   "source": [
    "    agg_list = [\n",
    "        [['time'], 'label', 'mean', [12*24,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,50,100,150], [1,2,3,4,5,10,15,20,25,7*12*17,14*12*17]],\n",
    "        [['time'], 'label', 'mean', [7*12*24], [7*12*24, 365*12*24]],\n",
    "        [['time'], 'label', 'mean', [7*12*24, 14*12*24, 30*12*24, 60*12*24, 180], [28]],\n",
    "        [['time'], 'label', 'std',  [7*12*24, 14*12*24, 30*12*24, 60*12*24], [28*12*17]],\n",
    "        [['time'], 'label', 'max', [365*12*24], [1*12*24]],\n",
    "        [['time'], 'label', 'mean', [7*12*24], [1*12*24]],\n",
    "        [['time'], 'calendrier', 'mean', [12*24,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,50], [1,2,3,4,5,10,15,20,25,7*12*17,14*12*17]],\n",
    "        \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtoJ-zWXENWz"
   },
   "outputs": [],
   "source": [
    "def update_features_one_day(df, time):\n",
    "    '''\n",
    "    update lag/rolling features for 5 min only\n",
    "    \n",
    "    used for submission creation, when the predictions for 28 days are performed recursively\n",
    "    '''\n",
    "    \n",
    "    from datetime import timedelta\n",
    "    agg_list = [\n",
    "        [['time'], 'label', 'mean', [12*24,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,50,100,150], [1,2,3,4,5,10,15,20,25,7*12*17,14*12*17]],\n",
    " \n",
    " \n",
    "    ]\n",
    "\n",
    "\n",
    "    for i, item in enumerate(agg_list):\n",
    "        print(i)\n",
    "        # unpack the parameters\n",
    "        groupby_cols, target_col, agg_function, rolling_windows, lags = item\n",
    "        groupby_col_str = \"_\".join(groupby_cols)\n",
    "        print(i+1)\n",
    "        print(df.time)\n",
    "        for lag in lags:\n",
    "            print(i+2)\n",
    "            for rolling_window in rolling_windows:\n",
    "                print(i+3)\n",
    "                col_name = agg_function+'_'+target_col+'_per_'+groupby_col_str+'_r'+str(rolling_window)+'_lag_'+str(lag)\n",
    "                print(i+4)\n",
    "                print(date-timedelta(hours=lag))\n",
    "                print(df[(df.time <= date-timedelta(hours=lag))])\n",
    "                df_window = df[(df.time <= date-timedelta(hours=lag)) & (df.time > date-timedelta(hours=lag+rolling_window))]\n",
    "                print(df_window)\n",
    "                print(i+5)\n",
    "                df_window_grouped = df_window.agg({target_col : agg_function}).reindex(df.loc[df.time==time])\n",
    "                print(i+6)\n",
    "                print((df_window[target_col].values))\n",
    "                df.loc[(df.time == time, col_name)] = df_window_grouped[target_col][0]\n",
    "\n",
    "\n",
    "                #df[col_name] = df[target_col].rolling(rolling_window).agg(agg_function).shift(lag)\n",
    "                \n",
    "    #df.loc[df.date == date, 'sell_price_norm'] = df.loc[df.date == date, 'sell_price'] / df.loc[df.date == date, 'max_sell_price_per_store_id_item_id_r365_lag_1']\n",
    "    #df.loc[df.date == date, 'sell_price_momentum'] = df.loc[df.date == date, 'sell_price'] / df.loc[df.date == date, 'mean_sell_price_per_store_id_item_id_r7_lag_1']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aa2AX0dv18Ym"
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3gS_g5hBLkEY"
   },
   "outputs": [],
   "source": [
    "## perform recursive prediction\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta  \n",
    "\n",
    "## create submission ID\n",
    "\n",
    "# the magic scaling factor that applies on every score prediction\n",
    "magic_factor = 1.0\n",
    "\n",
    "# the threshold below which the prediction is set to zero\n",
    "zero_threshold = 0.75\n",
    "\n",
    "# start_date and the number of dates to be predicted\n",
    "# this error analysis was performed when the validation set was not released. Please use 2016-04-24 to 2016-05-22 after the release\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "14ddbjkqPxrh",
    "outputId": "218716a3-a478-44fa-c092-a39c2b33286a"
   },
   "outputs": [],
   "source": [
    "sales_by_date_copy = sales_by_date.copy()\n",
    "r2=[]\n",
    "\n",
    "sales_by_date_copy['label_true'] = sales_by_date_copy['label']\n",
    "for k  in range(0,len(sales_by_date_copy['label_true'])):\n",
    "  if sales_by_date_copy['label_true'][k]<0.7:\n",
    "    sales_by_date_copy['label_true'][k] = 0\n",
    "#sales_by_date_copy['label_true'] = sales_by_date_copy['label_true'].clip(lower=0.4)\n",
    "sales_by_date_copy.reset_index(inplace=True)\n",
    "month_pred= []\n",
    "for j in range (1,30):\n",
    "  start_date = datetime(2015,10,j)\n",
    "  end_date = datetime(2015,10,j+1,0)\n",
    "  num_dates = 12*24\n",
    "# loop over days and generate score for each day\n",
    "  for i in range(num_dates):\n",
    "\n",
    "      date = start_date + 5*timedelta(minutes=i)\n",
    "      #print(date)\n",
    "      # compute the features on-the-fly b/c some features depend on predictions\n",
    "      X_features = sales_by_date_copy\n",
    "      X_pred = X_features[X_features['time'] == date][feature_cols]\n",
    "      #print(X_pred)\n",
    "      # generate predictions\n",
    "      y_pred = reg_model.predict(X_pred) * magic_factor\n",
    "      y_pred[y_pred <= zero_threshold] = 0\n",
    "      month_pred.append(y_pred)\n",
    "      # update predictions to the sales_by_date dataframe\n",
    "      sales_by_date_copy.loc[sales_by_date_copy['time'] == date, 'label'] = y_pred\n",
    "\n",
    "      ## Filter out the final table with our predictions\n",
    "\n",
    "  sales_by_date_copy2 = sales_by_date_copy.set_index('time').loc[start_date:end_date].reset_index()\n",
    "  #df_plot = sales_by_date_copy[(sales_by_date_copy.item_id == item_id) & (sales_by_date_copy.store_id == store_id)]\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  plt.plot(sales_by_date_copy2.time, sales_by_date_copy2.label.round(1), label='pred',color=\"orange\")\n",
    "  plt.plot(sales_by_date_copy2.time, sales_by_date_copy2.label_true.round(1), label='true',color=\"blue\")\n",
    "  plt.legend()\n",
    "  from sklearn.metrics import r2_score\n",
    "  from sklearn.metrics import mean_squared_error\n",
    "\n",
    "  plt.title('Test results for 24 hours prediction, R2_score:%f'%r2_score(sales_by_date_copy2['label'][0:-2],sales_by_date_copy2['label_true'][0:-2]))\n",
    "\n",
    "  \n",
    "  r2.append(r2_score(sales_by_date_copy2['label'][0:-2],sales_by_date_copy2['label_true'][0:-2]))\n",
    "  print(r2_score(sales_by_date_copy2['label'][0:-2],sales_by_date_copy2['label_true'][0:-2]))\n",
    "  print(mean_squared_error(sales_by_date_copy2['label'],sales_by_date_copy2['label_true']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Onmu6RkKQZiE",
    "outputId": "7b21a669-c486-4419-fceb-7b7e80d414f7"
   },
   "outputs": [],
   "source": [
    "np.mean(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TnQVLWpYXsAz",
    "outputId": "8f9aad41-956a-4889-8258-46142c7448c0"
   },
   "outputs": [],
   "source": [
    "sales_by_date_copy = sales_by_date.copy()\n",
    "\n",
    "\n",
    "sales_by_date_copy['label_true'] = sales_by_date_copy['label']\n",
    "for k  in range(0,len(sales_by_date_copy['label_true'])):\n",
    "  if sales_by_date_copy['label_true'][k]<0.7:\n",
    "    sales_by_date_copy['label_true'][k] = 0\n",
    "#sales_by_date_copy['label_true'] = sales_by_date_copy['label_true'].clip(lower=0.4)\n",
    "sales_by_date_copy.reset_index(inplace=True)\n",
    "month_pred= []\n",
    "for j in range (4,10):\n",
    "  start_date = datetime(2015,12,j)\n",
    "  end_date = datetime(2015,12,j+1,0)\n",
    "  num_dates = 12*24\n",
    "# loop over days and generate score for each day\n",
    "  for i in range(num_dates):\n",
    "\n",
    "      date = start_date + 5*timedelta(minutes=i)\n",
    "      #print(date)\n",
    "      # compute the features on-the-fly b/c some features depend on predictions\n",
    "      #X_features = sales_by_date_copy\n",
    "      X_features = sales_by_date_copy\n",
    "      X_pred = X_features[X_features['time'] == date][feature_cols]\n",
    "      #print(X_pred)\n",
    "      # generate predictions\n",
    "      y_pred = reg_model.predict(X_pred) * magic_factor\n",
    "      y_pred[y_pred <= zero_threshold] = 0\n",
    "      month_pred.append(y_pred)\n",
    "      # update predictions to the sales_by_date dataframe\n",
    "      sales_by_date_copy.loc[sales_by_date_copy['time'] == date, 'label'] = y_pred\n",
    "\n",
    "      ## Filter out the final table with our predictions\n",
    "\n",
    "  sales_by_date_copy2 = sales_by_date_copy.set_index('time').loc[start_date:end_date].reset_index()\n",
    "  #df_plot = sales_by_date_copy[(sales_by_date_copy.item_id == item_id) & (sales_by_date_copy.store_id == store_id)]\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  plt.plot(sales_by_date_copy2.time, sales_by_date_copy2.label.round(1), label='pred',color=\"orange\")\n",
    "  plt.plot(sales_by_date_copy2.time, sales_by_date_copy2.label_true.round(1), label='true',color=\"blue\")\n",
    "  plt.legend()\n",
    "  from sklearn.metrics import r2_score\n",
    "  from sklearn.metrics import mean_squared_error\n",
    "\n",
    "  plt.title('Test results for 24 hours prediction, R2_score:%f'%r2_score(sales_by_date_copy2['label'][0:-2],sales_by_date_copy2['label_true'][0:-2]))\n",
    "\n",
    "  \n",
    "  r2.append(r2_score(sales_by_date_copy2['label'][0:-2],sales_by_date_copy2['label_true'][0:-2]))\n",
    "  print(r2_score(sales_by_date_copy2['label'][0:-2],sales_by_date_copy2['label_true'][0:-2]))\n",
    "  print(mean_squared_error(sales_by_date_copy2['label'],sales_by_date_copy2['label_true']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uklJes1iYITe",
    "outputId": "570a8334-276c-4c0e-c2ef-b5bfba101f53"
   },
   "outputs": [],
   "source": [
    "np.mean(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fcYie57kw1o-",
    "outputId": "66dff9c8-febb-4d0f-d4d9-9c29e7da6d5e"
   },
   "outputs": [],
   "source": [
    "sales_by_date_copy = sales_by_date.copy()\n",
    "\n",
    "\n",
    "sales_by_date_copy['label_true'] = sales_by_date_copy['label']\n",
    "for k  in range(0,len(sales_by_date_copy['label_true'])):\n",
    "  if sales_by_date_copy['label_true'][k]<0.7:\n",
    "    sales_by_date_copy['label_true'][k] = 0\n",
    "#sales_by_date_copy['label_true'] = sales_by_date_copy['label_true'].clip(lower=0.4)\n",
    "sales_by_date_copy.reset_index(inplace=True)\n",
    "month_pred= []\n",
    "for j in range (1,30):\n",
    "  start_date = datetime(2015,11,j)\n",
    "  end_date = datetime(2015,11,j+1,0)\n",
    "  num_dates = 12*24\n",
    "# loop over days and generate score for each day\n",
    "  for i in range(num_dates):\n",
    "\n",
    "      date = start_date + 5*timedelta(minutes=i)\n",
    "      #print(date)\n",
    "      # compute the features on-the-fly b/c some features depend on predictions\n",
    "      X_features = sales_by_date_copy\n",
    "      X_pred = X_features[X_features['time'] == date][feature_cols]\n",
    "      #print(X_pred)\n",
    "      # generate predictions\n",
    "      y_pred = reg_model.predict(X_pred) * magic_factor\n",
    "      y_pred[y_pred <= zero_threshold] = 0\n",
    "      month_pred.append(y_pred)\n",
    "      # update predictions to the sales_by_date dataframe\n",
    "      sales_by_date_copy.loc[sales_by_date_copy['time'] == date, 'label'] = y_pred\n",
    "\n",
    "      ## Filter out the final table with our predictions\n",
    "\n",
    "  sales_by_date_copy2 = sales_by_date_copy.set_index('time').loc[start_date:end_date].reset_index()\n",
    "  #df_plot = sales_by_date_copy[(sales_by_date_copy.item_id == item_id) & (sales_by_date_copy.store_id == store_id)]\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  plt.plot(sales_by_date_copy2.time, sales_by_date_copy2.label.round(1), label='pred',color=\"orange\")\n",
    "  plt.plot(sales_by_date_copy2.time, sales_by_date_copy2.label_true.round(1), label='true',color=\"blue\")\n",
    "  plt.legend()\n",
    "  from sklearn.metrics import r2_score\n",
    "  from sklearn.metrics import mean_squared_error\n",
    "\n",
    "  plt.title('Test results for 24 hours prediction, R2_score:%f'%r2_score(sales_by_date_copy2['label'][0:-2],sales_by_date_copy2['label_true'][0:-2]))\n",
    "\n",
    "  \n",
    "  r2.append(r2_score(sales_by_date_copy2['label'][0:-2],sales_by_date_copy2['label_true'][0:-2]))\n",
    "  print(r2_score(sales_by_date_copy2['label'][0:-2],sales_by_date_copy2['label_true'][0:-2]))\n",
    "  print(mean_squared_error(sales_by_date_copy2['label'],sales_by_date_copy2['label_true']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j197tbVKxj5C",
    "outputId": "ee417e71-fe66-4ea0-a103-2bedbd1cdc2d"
   },
   "outputs": [],
   "source": [
    "np.mean(r2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LightGBM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
